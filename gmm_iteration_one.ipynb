{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project GMM testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Allen Phu\n",
    "- Kevin Yu\n",
    "- Saksham Rai\n",
    "- Rodrigo Lizaran-Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquick notes for future me\\n\\nthe labels are broken up into 0-25, one for each letter of the alphabet\\n\\ntraining data has 27,455 cases, test data has 7,172 cases\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT LIBRARIES AND QUICK INFO, IMPLEMENTATION OF FUNCTIONS\n",
    "\n",
    "# stolen from notebook L07 and gpt stuff\n",
    "# import stuff\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score\n",
    "from scipy import stats\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    \"\"\"Callable to pass to GridSearchCV that will use the BIC score.\"\"\"\n",
    "    # Make it negative since GridSearchCV expects a score to maximize\n",
    "    return -estimator.bic(X)\n",
    "\n",
    "'''\n",
    "quick notes for future me\n",
    "\n",
    "the labels are broken up into 0-25, one for each letter of the alphabet\n",
    "\n",
    "training data has 27,455 cases, test data has 7,172 cases\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTABLISH DATA\n",
    "\n",
    "train_data = pd.read_csv(\"data/sign_mnist_train.csv\")\n",
    "test_data = pd.read_csv(\"data/sign_mnist_test.csv\")\n",
    "true_labels = test_data['label']\n",
    "train_labels = train_data['label']\n",
    "\n",
    "# separate labels, data\n",
    "gmm_pred = pd.DataFrame(columns = ['true', 'pred'])\n",
    "\n",
    "predict_data = train_data.drop(['label'], axis=1)\n",
    "true_data = test_data.drop(['label'], axis=1)\n",
    "\n",
    "# fit data\n",
    "gmm = GaussianMixture(n_components=26, random_state=42)\n",
    "gmm.fit(predict_data)\n",
    "\n",
    "# make predictions \n",
    "gmm_pred['true'] = train_data['label']\n",
    "gmm_pred['pred'] = gmm.predict(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.07575039505807556\n",
      "Non-Adjusted Rand Score: 0.921382880523567\n"
     ]
    }
   ],
   "source": [
    "# calculate adjusted/non-adjusted rand index, silhouette coefficient, BIC, AIC\n",
    "adjusted_rand = adjusted_rand_score(gmm_pred['true'], gmm_pred['pred'])\n",
    "non_adjusted_rand = rand_score(gmm_pred['true'], gmm_pred['pred'])\n",
    "\n",
    "print(\"Adjusted Rand Score:\", adjusted_rand)\n",
    "print(\"Non-Adjusted Rand Score:\", non_adjusted_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130209446.69261551\n"
     ]
    }
   ],
   "source": [
    "# calculate BIC\n",
    "bic = gmm.bic(predict_data) # i'm too lazy to import\n",
    "print(bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64273331.26057597\n"
     ]
    }
   ],
   "source": [
    "# calculate AIC \n",
    "aic = gmm.aic(predict_data)\n",
    "print(aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05330493070205487\n"
     ]
    }
   ],
   "source": [
    "# calculate silhouette coefficient\n",
    "\n",
    "# gonna be non-predicted data, then predicted data\n",
    "silhouette = silhouette_score(predict_data, gmm_pred['pred'])\n",
    "\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_components\": range(21, 27),\n",
    "    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score\n",
    ")\n",
    "grid_search.fit(predict_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid_search.cv_results_)[\n",
    "    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n",
    "]\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"param_n_components\": \"Number of components\",\n",
    "        \"param_covariance_type\": \"Type of covariance\",\n",
    "        \"mean_test_score\": \"BIC score\",\n",
    "    }\n",
    ")\n",
    "df.sort_values(by=\"BIC score\").head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
