{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Sign Language Recognition\n",
    "\n",
    "# Names\n",
    "\n",
    "- Allen Phu\n",
    "- Kevin Yu\n",
    "- Saksham Rai\n",
    "- Rodrigo Lizaran-Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Our group originally came across Google’s “American Sign Language Fingerspelling Recognition” Kaggle competition <a name=\"one\"></a>[<sup>[1]</sup>](#onenote) while brainstorming for ideas for our project. We were intrigued by this idea as we were all interested in ML image processing in the first place, but the combination of making advancements in accessibility and image processing only solidified this topic as something we wanted to pursue for our final project. \n",
    "\n",
    "\n",
    "After some further research, we realized that although we were intrigued by the idea of mixing ML and ASL together, only one of us had prior experience with the MediaPipe library. As many prior writeups regarding the utilization of ML utilized the aforementioned MediaPipe library <a name=\"two\"></a>[<sup>[2]</sup>](#twonote) <a name=\"three\"></a>[<sup>[3]</sup>](#threenote), we decided to pivot towards the Sign Language MNIST dataset <a name=\"four\"></a>[<sup>[4]</sup>](#fournote) in order to make the project more digestible for ourselves. Previous studies have shown that using image recognition platforms in order to recognize ASL have already been successful, with an October 2023 study achieving 98.98% test accuracy <a name=\"five\"></a>[<sup>[5]</sup>](#fivenote) and an August 2022 study utilizing MediaPipe, Keras, and the Sign Language MNIST dataset achieving 95% training accuracy <a name=\"three\"></a>[<sup>[3]</sup>](#threenote). \n",
    "\n",
    "\n",
    "Regarding advancements in what’s been done for machine learning and ASL detection, Google has launched Project Shuwa in the past in order to bring awareness and teach more people about ASL <a name=\"two\"></a>[<sup>[2]</sup>](#twonote). One (of many components) of Project Shuwa is SignTown, an “interactive game that utilizes webcams and a web browser to help people learn about sign language and Deaf culture” <a name=\"two\"></a>[<sup>[2]</sup>](#twonote). Google has also made it easier to learn about both ASL and machine learning through the utilization of their “Teachable Machine” tool, where people can use a no-code approach to leverage machine learning to test a model’s ability to recognize ASL samples <a name=\"six\"></a>[<sup>[6]</sup>](#sixnote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are aiming to solve is the classification of ASL characters to provide an ability of communication from deaf people to those who do not understand ASL. With the 27,455 available training data samples, and 7172 samples allocated for testing, we can quantify the success rate by taking the proportion of correct classifications against incorrect classifications. Additionally, we plan to test our clusters by inputting test cases from making the signs on our webcams and observing if the results share a similar accuracy with the test data. This can be replicated as with the publicly available data, one can follow our methods of creating clusters for the data as well as our process in inputting the data from our webcamera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Our data will be composed of hand images. Each image will represent a letter of the American Sign Language.\n",
    "- Sign_mnist_test and Sign_mnist_train\n",
    "    - [Link](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data)\n",
    "    - 1570 variables/columns (785 each one) with 27455 observations in training data and 7172 observations in test data.\n",
    "    - Each of the 27455 observations represent an image and it is paired with a corresponding label on what hand sign the sample represents\n",
    "    - The images are represented as 784 pixels and each pixel ranges from 0-255 and these images are represented as greyscale versions of themselves\n",
    "    - There will not be any special handling/transformations for this data as we can immediately begin clustering with the numeric data values stored in each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since our project mainly focuses on clustering algorithms to identify similar features between different ASL hand sign images, we need to check how efficient our algorithm is in forming these clusters, and how distinct they are from other clusters. To achieve this, the best feature to use is the Adjusted Rand Score. Adjusted Rand Score is a measure of similarity between two clusterings, particularly focusing on pairs that are in agreement - belonging either to different or the same clusters. The score is a ratio of the number of agreeing pairs to the total number of pairs, giving a value between 0 (no agreement) and 1 (perfect agreement). In contrast to the Rand Score, the Adjusted Rand Score, as the name suggests “adjusts” the random score by accounting for clustering by chance. It does this by assigning close to 0 values to random clusterings and 1 for perfect matches. \n",
    "\n",
    "Beyond this, we will also test the model using traditional classification metrics in an adapted manner. After clustering, we assign the most common true label within each cluster to all its points, enabling us to apply metrics like accuracy, precision, recall, and the F1 score. This will be done to test the model for its accuracy in producing true and false positives/negatives. \n",
    "\n",
    "In addition to this, we will test our model's accuracy to handle real-time hand-sign interpretations. Non-informative ASL communicators will stand in front of the webcam and make one of the many signs. The model will predict the hand sign. This can be done by displaying the model's prediction on a screen in real time or logging the results for later analysis. We will compare the model's real-time predictions with the actual signs being performed to assess accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Our American Sign Language recognition research project utilises the MNIST dataset of hand signs. Even though this dataset has been fairly corroborated and extensively updated over the years; since our project has an application in helping the \"hard of hearing\", it's necessary to consider some possible ethical concerns:\n",
    "\n",
    "1. Clustering Algorithm Bias: The MNIST training data might lack diversity in hand shapes, skin tones, signing styles, or individual variations, resulting in the algorithm prioritising majority patterns, leading to inaccurate classifications for underrepresented groups. For instance, a dataset skewed towards younger signers might struggle with recognizing signs used more frequently by older adults. In addition to this, even with a perfect algorithm also, there might be implicit biases in the dataset which are being amplified by our clustering algorithm.For example, the data might contain more examples of right-handed signing, potentially impacting the model's ability to recognize signs performed with the left hand. While such conclusions are less often, they are important to take under consideration because the implications of such biases are significant. Misclassifications can lead to communication breakdowns, and misunderstandings, all of which on an extended period and a larger sample space may even lead to social exclusion for specific groups within the Deaf community. We aim to address these concerns by trialling and testing the usage of different clustering algorithms to mitigate the risk of false clustering. Hierarchical clustering, for instance, builds a hierarchy of clusters, allowing me to zoom in on specific signing styles or hand shapes potentially underrepresented in the data. Similarly, density-based methods like DBSCAN focus on identifying clusters based on data density, making them less susceptible to the influence of majority patterns that could amplify bias. By evaluating these alternative algorithms with diverse test sets and fairness metrics (accuracy, precision and F1 score), we will choose the model that best balances inclusivity and accuracy, ensuring equitable representation across all user groups.\n",
    "\n",
    "2. Data Privacy Issue: The MNIST Hand Sign dataset we are utilising contains 27,455 cases of test data. These are real-life pictures collected from different thousands of different people. Now, in high-security stakes, hand-pictures also serve as identifiers. Due to this reason, similar to any other dataset, our MNIST ASL dataset is also subject to the protection of the privacy of individuals. In addition to this, we will also have input data from voluntary non-ASL communicators to test the real-time interpretation feature of the app. We hope to address this by keeping the identity of the volunteers undisclosed. In addition, we do it by utilising a \"black-box\" way of allowing the user to interact with the model. Without giving them any information about existing ASL hand signs, we hope to test the model for its accuracy in identifying any or all signs that people make, closest to an actual ASL sign.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"onenote\"></a>1.[^](#one): Ashley Chow, Glenn Cameron, Manfred Georg, Mark Sherwood, Phil Culliton, Sam Sepah, Sohier Dane, Thad Starner. (2023). Google - American Sign Language Fingerspelling Recognition. Kaggle. https://kaggle.com/competitions/asl-fingerspelling<br> \n",
    "\n",
    "<a name=\"twonote\"></a>2.[^](#two): El Moujahid, K. (2021, December 1). Machine learning to make sign language more accessible. Google. https://blog.google/outreach-initiatives/accessibility/ml-making-sign-language-more-accessible/<br> \n",
    "\n",
    "<a name=\"threenote\"></a>3.[^](#three): Garimella, M. (2022, August 23). Sign Language Recognition with Advanced Computer Vision. Medium. https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442<br> \n",
    "\n",
    "<a name=\"fournote\"></a>4.[^](#four): tecperson. (October 2017). Sign Language MNIST. Kaggle. https://www.kaggle.com/datasets/datamunge/sign-language-mnist<br> \n",
    "\n",
    "<a name=\"fivenote\"></a>5.[^](#five): Pathan, R. K., Biswas, M., Yasmin, S., Khandaker, M. U., Salman, M., & Youssef, A. A. F. (2023). Sign language recognition using the fusion of image and hand landmarks through multi-headed convolutional neural network. Scientific Reports, 13(1), 16975. https://doi.org/10.1038/s41598-023-43852-x<br> \n",
    "\n",
    "<a name=\"sixnote\"></a>6.[^](#six): Chen, Y. (2023, December 29). Learning American Sign Language (ASL) with Google’s Teachable Machine: A No-Code Experiment. Medium. https://medium.com/@dynotes/breaking-barriers-using-googles-no-code-approach-for-sign-language-recognition-and-learning-fc92ae16522c#bypass<br> \n",
    "\n",
    "\"<a name=\"MLTeaching\"></a>7.[^](#MLTeachingNote) Google open Teachable Machine. https://teachablemachine.withgoogle.com/train/tiny_image\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
