{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Allen Phu\n",
    "- Kevin\n",
    "- Saksham\n",
    "- Rodrigo Lizaran-Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Being provided with a dataset of many characters in American Sign Language, we thought it would be interesting to utilize many of the clustering techniques taught in this course to generate clusters of the ASL characters and from there, have our models be optimized enough to be able to use our web camera and create our own signs, the model should be able to correctly assign it to a cluster and return the character. Our data is represented as a vector of pixel intensities that range from 0-255 (single dimensional as it is greyscale). With this, we would run a clustering algorithm on the greyscale images. Next, assuming the clusters are accurate, we would transform our webcam image with a student forming a sign, and input the greyscale image into our clustering algorithm where it would then identify the proper character. To measure performance, we would allocate a portion of the dataset for testing and based off the proportion of correct classifications vs incorrect classifications, we would return an accuracy percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Our group originally came across Google’s “American Sign Language Fingerspelling Recognition” Kaggle competition <a name=\"Ashleynote\"></a>[<sup>[1]</sup>](#Ashley) while brainstorming for ideas for our project. We were intrigued by this idea as we were all interested in ML image processing in the first place, but the combination of making advancements in accessibility and image processing only solidified this topic as something we wanted to pursue for our final project. \n",
    "\n",
    "After some further research, we realized that although we were intrigued by the idea of mixing ML and ASL together, only one of us had prior experience with the MediaPipe library. As many prior writeups regarding the utilization of ML utilized the aforementioned MediaPipe library <a name=\"ElMoujahid\"></a>[<sup>[2]</sup>](#ElMoujahid) <a name=\"Garimella\"></a>[<sup>[3]</sup>](#GarimellaNote), we decided to pivot towards the Sign Language MNIST dataset <a name=\"tecperson\"></a>[<sup>[4]</sup>](#tecpersonNote) in order to make the project more digestible for ourselves. Previous studies have shown that using image recognition platforms in order to recognize ASL have already been successful, with an October 2023 study achieving 98.98% test accuracy <a name=\"Pathan\"></a>[<sup>[5]</sup>](#PathanNote) and an August 2022 study utilizing MediaPipe, Keras, and the Sign Language MNIST dataset achieving 95% training accuracy <a name=\"Garimella\"></a>[<sup>[3]</sup>](#GarimellaNote). \n",
    "\n",
    "Regarding advancements in what’s been done for machine learning and ASL detection, Google has launched Project Shuwa in the past in order to bring awareness and teach more people about ASL <a name=\"ElMoujahid\"></a>[<sup>[2]</sup>](#ElMoujahid). One (of many components) of Project Shuwa is SignTown, an “interactive game that utilizes webcams and a web browser to help people learn about sign language and Deaf culture” <a name=\"ElMoujahid\"></a>[<sup>[2]</sup>](#ElMoujahid). Google has also made it easier to learn about both ASL and machine learning through the utilization of their “Teachable Machine” tool, where people can use a no-code approach to leverage machine learning to test a model’s ability to recognize ASL samples <a name=\"Chen\"></a>[<sup>[6]</sup>](#ChenNote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are aiming to solve is the classification of ASL characters to provide an ability of communication from deaf people to those who do not understand ASL. With the 27,455 available training data samples, and 7172 samples allocated for testing, we can quantify the success rate by taking the proportion of correct classifications against incorrect classifications. Additionally, we plan to test our clusters by inputting test cases from making the signs on our webcams and observing if the results share a similar accuracy with the test data. This can be replicated as with the publicly available data, one can follow our methods of creating clusters for the data as well as our process in inputting the data from our webcamera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Our data will be composed of hand images. Each image will represent a letter of the American Sign Language.\n",
    "- Sign_mnist_test and Sign_mnist_train\n",
    "    - [Link](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data)\n",
    "    - 1570 variables/columns (785 each one) with 27455 observations in training data and 7172 observations in test data.\n",
    "    - Each of the 27455 observations represent an image and it is paired with a corresponding label on what hand sign the sample represents\n",
    "    - The images are represented as 784 pixels and each pixel ranges from 0-255 and these images are represented as greyscale versions of themselves\n",
    "    - There will not be any special handling/transformations for this data as we can immediately begin clustering with the numeric data values stored in each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To solve the problem of clustering ASL (American Sign Language) hand sign images using unsupervised machine learning and achieving high accuracy in real-time sign language interpretation through a camera, we can employ the following solution:\n",
    "\n",
    "***Solution Description***\n",
    "\n",
    "1. Preprocessing:\n",
    "    - Resize images to a consistent dimension to ensure uniformity.\n",
    "    - Transform pixel values into 1D array for each image.\n",
    "\n",
    "2. Feature Extraction:\n",
    "    - Utilize a dimensionality reduction technique such as PCA, to extract essential features and reduce computational complexity.\n",
    "\n",
    "3. Unsupervised Clustering:\n",
    "    - Apply clustering algortihm such as k-means, DBSCAN to group similar hand sign images together. (The choice of k can be determined by an Elbow method or Silhouette Score)\n",
    "\n",
    "4. Cluster Label Assigment:\n",
    "    - Assign labels to the clusters based on the majority class of the images within each cluster.\n",
    "\n",
    "5. Real-time Sign Language Interpretation:\n",
    "    - Use a camera to capture frames in real-time.\n",
    "    - Preprocess each frame similarly to the training data.\n",
    "    - Apply the trained clustering model to assign a cluster label to the current frame.\n",
    "    - Map the cluster label to the corresponding ASL letter using the assigned labels from the training phase.\n",
    "\n",
    "6. Testing and Validation:\n",
    "    - Split the dataset into training and testing sets to evaluate the model's performance.\n",
    "    - Use metrics like accuracy, precision, recall, and F1-score to assess the clustering model's performance on the testing set.\n",
    "    - For real-time interpretation, validate the accuracy by comparing the predicted letters to the ground truth ASL letters in a controlled environment.\n",
    "\n",
    "7. Benchmark Model:\n",
    "    - Google open Teachable Machine<a name=\"MLTeaching\"></a>[<sup>[7]</sup>](#MLTeachingNote) allow to upload a video or photos of our hands making the ASL, it will train the model and print out accuracy of the model.\n",
    "\n",
    "\n",
    "***Implementation Details***\n",
    "- Libraries: \n",
    "    - Use popular machine learning libraries such as scikit-learn for clustering algorithms, OpenCV for image processing, and NumPy for numerical operations.\n",
    "\n",
    "- Function Calls:\n",
    "    - Use scikit-learn's implementation of K-means or other clustering algorithms.\n",
    "    - Apply PCA for dimensionality reduction using scikit-learn's PCA module.\n",
    "\n",
    "- Real-time Integration:\n",
    "    - Utilize OpenCV for capturing and processing camera frames.\n",
    "    - Implement a real-time loop to continuously process frames and display the predicted ASL letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a team, we are committed to maintaining a high level of collaboration, professionalism, and respect among all members. Our shared expectations for one another are as follows:\n",
    "\n",
    "* **Active and Respectful Communication**: We value open and active communication. Team members should feel comfortable expressing their thoughts, ideas, and concerns. We will listen actively and respectfully to each other.\n",
    "\n",
    "* **Idea Exchange**: Everyone is welcome to contribute their thoughts and suggestions. We will consider each other's ideas with respect and appreciation.\n",
    "\n",
    "* **Continuous Collaboration**: If any team member has ideas or encounters challenges, please share them directly through our Discord communication channel as soon as possible. We believe that open and timely communication is essential to address issues promptly and efficiently.\n",
    "\n",
    "* **Timely Task Completion**: We understand the importance of meeting deadlines. Team members are expected to complete their assigned sections on time. If you foresee challenges in meeting a deadline, please communicate this at least two days in advance so that we can collectively find solutions.\n",
    "\n",
    "* **Equal Work Distribution**: The workload will be distributed equitably. All team members are expected to contribute to the final project, ensuring that no one bears an undue burden.\n",
    "\n",
    "\n",
    "\n",
    "**Rodrigo Lizaran-Molina**: Data, Proposed Solution\n",
    "\n",
    "**Allen Phu**: Background\n",
    "\n",
    "**Kevin Y**: Abstract, Data and Problem Statement\n",
    "\n",
    "**Saksham Rai**: Evaluation Metrics, Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/18  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/19  |  4 PM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/20  |  4 PM | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/25  |  6 PM | Import & Wrangle Data ,do some EDA  | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 3/1   | 12 PM | Finalize wrangling/EDA; Begin programming for project  | Discuss/edit project code; Complete project |\n",
    "| 3/19  | 12 PM | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"Ashleynote\"></a>1.[^](#Ashley) Ashley Chow, Glenn Cameron, Manfred Georg, Mark Sherwood, Phil Culliton, Sam Sepah, Sohier Dane, Thad Starner. (2023). Google - American Sign Language Fingerspelling Recognition. Kaggle. https://kaggle.com/competitions/asl-fingerspelling<br> \n",
    "<a name=\"ElMoujahid\"></a>2.[^](#ElMoujahid) El Moujahid, K. (2021, December 1). Machine learning to make sign language more accessible. Google. https://blog.google/outreach-initiatives/accessibility/ml-making-sign-language-more-accessible/<br> \n",
    "<a name=\"Garimella\"></a>3.[^](#GarimellaNote) Garimella, M. (2022, August 23). Sign Language Recognition with Advanced Computer Vision. Medium. https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442<br> \n",
    "<a name=\"tecperson\"></a>4.[^](#tecpersonNote) tecperson. (October 2017). Sign Language MNIST. Kaggle. https://www.kaggle.com/datasets/datamunge/sign-language-mnist<br> \n",
    "<a name=\"Pathan\"></a>5.[^](#PathanNote) Pathan, R. K., Biswas, M., Yasmin, S., Khandaker, M. U., Salman, M., & Youssef, A. A. F. (2023). Sign language recognition using the fusion of image and hand landmarks through multi-headed convolutional neural network. Scientific Reports, 13(1), 16975. https://doi.org/10.1038/s41598-023-43852-x<br> \n",
    "<a name=\"Chen\"></a>6.[^](#ChenNote) Chen, Y. (2023, December 29). Learning American Sign Language (ASL) with Google’s Teachable Machine: A No-Code Experiment. Medium. https://medium.com/@dynotes/breaking-barriers-using-googles-no-code-approach-for-sign-language-recognition-and-learning-fc92ae16522c#bypass\n",
    "<a name=\"MLTeaching\"></a>7.[^](#MLTeachingNote) Google open Teachable Machine. https://teachablemachine.withgoogle.com/train/tiny_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
